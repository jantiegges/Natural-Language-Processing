\documentclass[12pt]{article}

% Packages for formatting
\usepackage{titlesec}
\usepackage{geometry}
\usepackage{filecontents}

% Define page margins
\geometry{top=1in, bottom=1in, left=1in, right=1in}

% Define title page formatting
\titleformat{\section}[block]{\normalfont\large\bfseries}{\thesection}{1em}{}
\title{\textbf{COMP 550: Programming Assignment 1}}
\date{\today}
\author{Jan Tiegges}
\renewcommand{\thesection}{\arabic{section}}

\begin{document}

\maketitle

\section{Introduction}
% problem setup
% problem description
% s it possible to distinguish real from fake facts about animals using linear classifiers?
% does the choice of the linear classifier matter?
In this assignment, the use of different linear classifiers as well as the impact of different choices regarding preprocessing, feature extraction and hyperparameters was investigated based on a toy experiment. The task was to classify short statements about animals whether it is a true or an imaginary one. The primary research question was to find out whether linear classifiers are suitable for such tasks and which effect the choice of a classifier has.


\section{Methodology}
% your dataset generation and experimental procedure
% the range of parameter settings that you tried

% how did I generate the data set
% how big is the data set
% what pre-processing parameter did I choose
% which feature extraction mechanisms did I choose
% what classification models have been used
% what parameters have been chosen with CV
The experiments used data generated by ChatGPT by providing clear guidelines and examples to maintain quality. 230 statements were obtained for 23 animals (10 facts and fakes each) and the combination with additional data from students resulted in 1030 statements for each category. For preprocessing, the data was split into individual tokens and then optionally lemmatization and porter stemming were applied and the stop words were removed. For feature extraction, the data was split into an 80/20 train/test split and processed using a count vectorizer with unigrams. Classification was performed using Naive Bayes (NB),Logistic Regression (LR) and Linear Support Vector Machines (SVM). For all preprocessing options, a 5-fold cross-validation optimized the regularization parameter $C$ for LR and SVM and the smoothing parameter $\alpha$ for NB. The LR solver was "Saga" with a maximum number of 5000 iterations. After that another 5-fold was performed finding the best model for each combination of preprocessing options. The best models was then evaluated on the test set.

\section{Results}
% report on the performance in terms of accuracy
% compare for each classification model individually
% table with best and worst scores for all models, preprocessing techniques and feature extraction mechanisms

% preprocessing
% models 

% general observations


\section{Discussion}
% speculate on the successes and failures of the models.


% answer research questions
% it is possible to classify
% choice of linear classifier matters 
% fun facts about missclassifications 
% interesting facts about data (why could it perform so good)

% limitations
% small dataset. Only generated by gpt-3.5, therefore not so much variation in distribution
% as it was generated with ChatGPT iteratively, facts were based on the facts before, which probably made them even more similar

% think about how much you can generalize the conclusions of your experiments to the overall problem of separating real vs. fake facts in general. What assumptions have we made that are reasonable or not reasonable?
not general. The strongest assumption is that the type of language/words used indicates wether it is a fact or not. This might be true for very easy cases, but if facts would sound more realistic, then the model does not work anymore. Also, it does not take into account the context of the animal, which also is a strong assumption. In the case where the made-up facts are actually very creative and non-sense like, while the facts are more neutral scientifc ones, then these assumptions actually work out reasonable and make the model perform well.

\section{Conclusion}
gave some good hints on the interplay between choosing the right pre-processing, feature extraction and classification models for such a task. Generally one should first conduct some data analysis to get a good understanding of the data which can be used as prior knowledge to make some model decisions upfront. This task however could already show how strong simple mechanisms and linear classification already work on data sets like this.

% References section
\bibliographystyle{plain}
\bibliography{references}

\end{document}